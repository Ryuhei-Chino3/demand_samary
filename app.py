import streamlit as st
import pandas as pd
import io

st.title("PPAãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¢ãƒ—ãƒªï¼ˆCSV / Excel å¯¾å¿œï¼‰")

uploaded_files = st.file_uploader(
    "CSV ã¾ãŸã¯ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰",
    type=["xlsx", "csv"],
    accept_multiple_files=True
)

def try_read_csv(file):
    encodings = ["utf-8", "cp932", "iso-8859-1"]
    for enc in encodings:
        try:
            file.seek(0)
            return pd.read_csv(file, encoding=enc)
        except Exception:
            continue
    raise ValueError("èª­ã¿è¾¼ã¿å¯èƒ½ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã¯ã‚ã‚Šã¾ã›ã‚“")

required_cols = ["year", "month", "date", "time", "è²·é›»é›»åŠ›é‡(kWh)", "å£²é›»é›»åŠ›é‡(kWh)", "ç™ºé›»é›»åŠ›é‡(kWh)", "æ¶ˆè²»é›»åŠ›é‡(kWh)"]

if uploaded_files:
    dfs = []
    for file in uploaded_files:
        file_name = file.name.lower()

        st.write(f"--- ãƒ•ã‚¡ã‚¤ãƒ«å: {file.name} ---")

        try:
            if file_name.endswith(".xlsx"):
                df = pd.read_excel(file)
            elif file_name.endswith(".csv"):
                df = try_read_csv(file)
            else:
                st.warning(f"æœªå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_name}")
                continue
        except Exception as e:
            st.error(f"{file_name} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue

        st.write("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç›´å¾Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­5è¡Œï¼‰")
        st.dataframe(df.head())

        missing = [col for col in required_cols if col not in df.columns]
        if missing:
            st.warning(f"{file_name} ã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {missing}")
            continue

        try:
            df["year"] = df["year"].astype(int)
            df["month"] = df["month"].astype(int)
            df["date"] = df["date"].astype(int)

            month_str = df["month"].astype(str).str.zfill(2)
            date_str = df["date"].astype(str).str.zfill(2)

            df["datetime"] = pd.to_datetime(
                df["year"].astype(str) + "-" + month_str + "-" + date_str + " " + df["time"].astype(str),
                format="%Y-%m-%d %H:%M:%S",
                errors="coerce"
            )

            st.write("datetimeåˆ—ã®ä¸­èº«ï¼ˆå…ˆé ­5è¡Œï¼‰")
            st.dataframe(df["datetime"].head())

            null_dt_count = df["datetime"].isna().sum()
            st.write(f"datetimeç”Ÿæˆå¤±æ•—ï¼ˆNaTï¼‰ã®è¡Œæ•°: {null_dt_count}")

            df = df.dropna(subset=["datetime"])

            dfs.append(df)
        except Exception as e:
            st.error(f"{file_name} ã® datetime ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    if not dfs:
        st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        value_cols = ["è²·é›»é›»åŠ›é‡(kWh)", "å£²é›»é›»åŠ›é‡(kWh)", "ç™ºé›»é›»åŠ›é‡(kWh)", "æ¶ˆè²»é›»åŠ›é‡(kWh)"]

        # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã®datetimeã§outerãƒãƒ¼ã‚¸ã—ã¦å¹³å‡åŒ–
        merged_df = None
        for df in dfs:
            df_small = df[["datetime"] + value_cols].copy()
            if merged_df is None:
                merged_df = df_small
            else:
                merged_df = pd.merge(
                    merged_df,
                    df_small,
                    on="datetime",
                    how="outer",
                    suffixes=("", "_dup")
                )

        if merged_df is None or merged_df.empty:
            st.error("ãƒ‡ãƒ¼ã‚¿çµåˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            avg_df = merged_df.copy()
            for col in value_cols:
                avg_cols = [c for c in merged_df.columns if c.startswith(col)]
                avg_df[col] = merged_df[avg_cols].mean(axis=1)

            # 30åˆ†å€¤ã‚·ãƒ¼ãƒˆç”¨
            base_df = dfs[0].copy()
            output_cols = base_df.columns.tolist()

            final_30min_df = pd.merge(
                base_df.drop(columns=value_cols),
                avg_df[["datetime"] + value_cols],
                on="datetime",
                how="left"
            )
            final_30min_df = final_30min_df[output_cols]

            # ã‚µãƒãƒªãƒ¼ã‚·ãƒ¼ãƒˆç”¨
            summary_df = pd.concat(dfs)
            summary_df["year"] = summary_df["year"].astype(int)
            summary_df["month"] = summary_df["month"].astype(int)
            summary_grouped = summary_df.groupby(["year", "month"])[value_cols].sum().reset_index()

            # å¹´é–“å¹³å‡ã‚·ãƒ¼ãƒˆç”¨
            # å…¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ†concatã—ã€datetimeã‹ã‚‰æ™‚é–“éƒ¨åˆ†ã ã‘æŠ½å‡ºã—ã¦ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã—å¹³å‡å€¤ã‚’è¨ˆç®—
            all_concat = pd.concat(dfs)
            all_concat["time_only"] = all_concat["datetime"].dt.strftime("%H:%M:%S")
            annual_avg_df = all_concat.groupby("time_only")[value_cols].mean().reset_index()

            # Excelå‡ºåŠ›
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                final_30min_df.to_excel(writer, index=False, sheet_name="30åˆ†å€¤")
                summary_grouped.to_excel(writer, index=False, sheet_name="ã‚µãƒãƒªãƒ¼")
                annual_avg_df.to_excel(writer, index=False, sheet_name="å¹´é–“å¹³å‡")

            st.success("âœ… é›†è¨ˆå®Œäº†ï¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            st.download_button(
                label="ğŸ“¥ PPAãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼.xlsx ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=output.getvalue(),
                file_name="PPAãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
else:
    st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
